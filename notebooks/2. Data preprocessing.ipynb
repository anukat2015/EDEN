{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "import pickle_util\n",
    "import nltk\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%pylab inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# config\n",
    "# g_name = 'dev'\n",
    "# g_name = 'train'\n",
    "g_name = 'full_clean'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "f_name = g_name\n",
    "data = pickle_util.load_obj(f_name)\n",
    "# docs = data['hits']['hits']\n",
    "docs = data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "stopwords = nltk.corpus.stopwords.words('english')\n",
    "stemmer = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# http://brandonrose.org/clustering\n",
    "\n",
    "def tokenize_and_stem(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    stems = [stemmer.stem(t) for t in filtered_tokens]\n",
    "    return stems\n",
    "\n",
    "\n",
    "def tokenize_only(text):\n",
    "    # first tokenize by sentence, then by word to ensure that punctuation is caught as it's own token\n",
    "    tokens = [word.lower() for sent in nltk.sent_tokenize(text) for word in nltk.word_tokenize(sent)]\n",
    "    filtered_tokens = []\n",
    "    # filter out any tokens not containing letters (e.g., numeric tokens, raw punctuation)\n",
    "    for token in tokens:\n",
    "        if re.search('[a-zA-Z]', token):\n",
    "            filtered_tokens.append(token)\n",
    "    return filtered_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<type 'list'>\n"
     ]
    }
   ],
   "source": [
    "print type(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab len: 3107468\n"
     ]
    }
   ],
   "source": [
    "# create a vocabulary\n",
    "totalvocab_stemmed = []\n",
    "totalvocab_tokenized = []\n",
    "\n",
    "for d in docs:\n",
    "    \n",
    "#     allwords_stemmed = tokenize_and_stem(d['_source']['content'])\n",
    "    allwords_stemmed = tokenize_and_stem(d['content'])\n",
    "    totalvocab_stemmed.extend(allwords_stemmed)\n",
    "    \n",
    "#     allwords_tokenized = tokenize_only(d['_source']['content'])\n",
    "    allwords_tokenized = tokenize_only(d['content'])\n",
    "    totalvocab_tokenized.extend(allwords_tokenized)\n",
    "    \n",
    "vocab = pd.DataFrame({'words': totalvocab_tokenized}, index=totalvocab_stemmed)\n",
    "print \"vocab len: \" + str(vocab.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8253, 68)\n"
     ]
    }
   ],
   "source": [
    "# tf.idf article content\n",
    "docs_content = [d['content'] for d in docs]\n",
    "content_vectorizer = TfidfVectorizer(stop_words='english', max_df=0.8, max_features=200000, use_idf=True,\n",
    "                            min_df=0.2, tokenizer=tokenize_and_stem, ngram_range=(1,3))\n",
    "X_content = content_vectorizer.fit_transform(docs_content)\n",
    "terms_content = content_vectorizer.get_feature_names()\n",
    "print X_content.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8253, 154517)\n"
     ]
    }
   ],
   "source": [
    "# tf.idf article signal-entities (surface-form)\n",
    "docs_entities = []\n",
    "for d in docs:\n",
    "    try:\n",
    "        entities = d['signal-entities']\n",
    "        d['signal-entities-text'] = \" \".join([entity['surface-form'] for entity in entities])\n",
    "    except Exception:\n",
    "        print \"except\", Exception\n",
    "#         d['signal-entities'] = []\n",
    "        d['signal-entities-text'] = \"\"\n",
    "        \n",
    "    docs_entities.append(d['signal-entities-text'])\n",
    "    \n",
    "entity_vectorizer = TfidfVectorizer(use_idf=True, ngram_range=(1,2))\n",
    "X_entities = entity_vectorizer.fit_transform(docs_entities)\n",
    "terms_entities = entity_vectorizer.get_feature_names()\n",
    "print X_entities.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# save objects: vocab, X_content (tf.idf for content)\n",
    "# X_entity (tf.idf for entities), ..\n",
    "pickle_util.save_obj(vocab, 'vocab_'+g_name)\n",
    "pickle_util.save_obj(X_content, 'X_content_'+g_name)\n",
    "pickle_util.save_obj(X_entities, 'X_entities_'+g_name)\n",
    "pickle_util.save_obj(terms_content, 'terms_content_'+g_name)\n",
    "pickle_util.save_obj(terms_entities, 'terms_entities_'+g_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
